<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publication | Zeyu Zhang | BIGAI </title> <meta name="author" content="Zeyu Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="Cognitive Robot, Manipulation, Robot Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tooschoolforcool.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zeyu Zhang | BIGAI </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publication <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publication</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <h2 id="pre-print">Pre-Print</h2> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#808080"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Pre-Print</a> </abbr> <figure> <picture> <img src="/papers/2025-ikdiffuser/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-ikdiffuser/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025ikdiffuser" class="col-sm-8"> <div class="title">IKDiffuser: A Generative Inverse Kinematics Solver for Multi-arm Robots via Diffusion Model</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Kinematics</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Multi-arm Systems</code> </div> <div class="author"> <em>Zeyu Zhang</em> and <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† denotes corresponding author"> </i> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.arxiv.org/pdf/2506.13087" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/papers/ikdiffuser" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has primarily been successful with single serial manipulators. For multi-arm robotic systems, IK remains challenging due to complex self-collisions, coupled joints, and high-dimensional redundancy. These complexities make traditional IK solvers slow, prone to failure, and lacking in solution diversity. In this paper, we present IKDiffuser, a diffusion-based model designed for fast and diverse IK solution generation for multi-arm robotic systems. IKDiffuser learns the joint distribution over the configuration space, capturing complex dependencies and enabling seamless generalization to multi-arm robotic systems of different structures. In addition, IKDiffuser can incorporate additional objectives during inference without retraining, offering versatility and adaptability for task-specific requirements. In experiments on 6 different multi-arm systems, the proposed IKDiffuser achieves superior solution accuracy, precision, diversity, and computational efficiency compared to existing solvers. The proposed IKDiffuser framework offers a scalable, unified approach to solving multi-arm IK problems, facilitating the potential of multi-arm robotic systems in real-time manipulation tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025ikdiffuser</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{IKDiffuser: A Generative Inverse Kinematics Solver for Multi-arm Robots via Diffusion Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Jiao, Ziyuan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2506.13087}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <h2 id="all-publications">All Publications</h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">T-CE</abbr> <figure> <picture> <img src="/papers/2025-tce-teleop/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-tce-teleop/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wu2025vr" class="col-sm-8"> <div class="title">A VR-Based Robotic Teleoperation System With Haptic Feedback and Adaptive Collision Avoidance</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teleoperation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tactile</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> </div> <div class="author"> Fan Wu<sup>*</sup>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*†</sup></a>, <a href="https://mrliwanlin.github.io/" rel="external nofollow noopener" target="_blank">Wanlin Li<sup>*†</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, Hang Li, Jiahao Wu, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a>, and Shaopeng Dong <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Transactions on Consumer Electronics (T-CE)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2025-tce-teleop/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Robotic teleoperation systems enable humans to control robots remotely. Recent advancements in VR have transformed teleoperation into immersive, intuitive platforms that improve human-machine synergy. However, existing VR-based teleoperation systems face challenges such as under-informed shared control, limited tactile feedback, and computational inefficiencies, which hinder their effectiveness in complex, cluttered scenarios. This paper introduces a novel VR-based teleoperation system incorporating haptic feedback and adaptive collision avoidance. The system tracks human hand trajectories via an VR-based handheld device and provides tactile feedback from a robotic gripper. Through the proposed tele-MPPI method, the system anticipates robot motions, adaptively adjusts trajectories to avoid obstacles, and maintains computational efficiency, enabling real-time operation at 20 Hz. Simulations and experiments demonstrate the system’s ability to grasp delicate objects without damage and to navigate cluttered environments by mitigating collision risks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wu2025vr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A VR-Based Robotic Teleoperation System With Haptic Feedback and Adaptive Collision Avoidance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Fan and Jiao, Ziyuan and Li, Wanlin and Zhang, Zeyu and Li, Hang and Wu, Jiahao and Jia, Baoxiong and Dong, Shaopeng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Consumer Electronics (T-CE)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6610f2"> <a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank">RA-L</a> </abbr> <figure> <picture> <img src="/papers/2025-ral-m3bench/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-ral-m3bench/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025m3bench" class="col-sm-8"> <div class="title">M3Bench: Benchmarking Whole-Body Motion Generation for Mobile Manipulation in 3D Scenes</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Mobile Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Motion Generation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Benchmark</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sixu-yan.github.io/" rel="external nofollow noopener" target="_blank">Sixu Yan<sup>*</sup></a>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, Zaijin Wang, <a href="https://xwcv.github.io/" rel="external nofollow noopener" target="_blank">Xinggang Wang</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.06678" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/TwJQnRm663M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="#" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="/papers/m3bench" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We propose M3Bench, a new benchmark for whole-body motion generation in mobile manipulation tasks. Given a 3D scene context, M3Bench requires an embodied agent to reason about its configuration, environmental constraints, and task objectives to generate coordinated whole-body motion trajectories for object rearrangement. M3Bench features 30,000 object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly developed M3BenchMaker, an automatic data generation tool that produces whole-body motion trajectories from high-level task instructions using only basic scene and robot information. Our benchmark includes various task splits to evaluate generalization across different dimensions and leverages realistic physics simulation for trajectory assessment. Extensive evaluation analysis reveals that state-of-the-art models struggle with coordinating base-arm motion while adhering to environmental and task-specific constraints, underscoring the need for new models to bridge this gap. By releasing M3Bench and M3BenchMaker at https://zeyuzhang.com/papers/m3bench, we aim to advance robotics research toward more adaptive and capable mobile manipulation in diverse, real-world environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2025m3bench</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{M3Bench: Benchmarking Whole-Body Motion Generation for Mobile Manipulation in 3D Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Yan, Sixu and Han, Muzhi and Wang, Zaijin and Wang, Xinggang and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#fd7e14"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" rel="external nofollow noopener" target="_blank">T-PAMI</a> </abbr> <figure> <picture> <img src="/papers/2025-tpami-m2diffuser/thumbnail.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-tpami-m2diffuser/thumbnail.jpeg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yan2025m2diffuser" class="col-sm-8"> <div class="title">M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Mobile Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Motion Generation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Trajectory Optimization</code> </div> <div class="author"> <a href="https://sixu-yan.github.io/" rel="external nofollow noopener" target="_blank">Sixu Yan</a>, <em>Zeyu Zhang</em>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, Zaijin Wang, Qi Xie, Zhitian Li, Zhehan Li, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a>, <a href="https://xwcv.github.io/" rel="external nofollow noopener" target="_blank">Xinggang Wang<sup>†</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.11402" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/T7kpDifRtfk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/m2diffuser/M2Diffuser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://m2diffuser.github.io/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yan2025m2diffuser</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Sixu and Zhang, Zeyu and Han, Muzhi and Wang, Zaijin and Xie, Qi and Li, Zhitian and Li, Zhehan and Liu, Hangxin and Wang, Xinggang and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2025-icra-comebot/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-icra-comebot/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhi2024closed" class="col-sm-8"> <div class="title">Closed-loop Open-vocabulary Mobile Manipulation with GPT-4v</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">LLM</code> </div> <div class="author"> Peiyuan Zhi<sup>*</sup>, Zhiyuan Zhang<sup>*</sup>, Yu Zhao, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, <em>Zeyu Zhang</em>, Zhitian Li, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia<sup>†</sup></a>, and Siyuan Huang<sup>†</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2404.10220" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://come-robot.github.io/static/videos/icra_demov2.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://come-robot.github.io/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate ( 35%) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robot’s design facilitates failure recovery, free-form instruction following, and long-horizon task planning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhi2024closed</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Closed-loop Open-vocabulary Mobile Manipulation with GPT-4v}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhi, Peiyuan and Zhang, Zhiyuan and Zhao, Yu and Han, Muzhi and Zhang, Zeyu and Li, Zhitian and Jiao, Ziyuan and Jia, Baoxiong and Huang, Siyuan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#20c997"> <a href="https://onlinelibrary.wiley.com/journal/15564967" rel="external nofollow noopener" target="_blank">JFR</a> </abbr> <figure> <picture> <img src="/papers/2025-jfr-pr2/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-jfr-pr2/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024pr2" class="col-sm-8"> <div class="title">PR2: A Physics-and Photo-realistic Testbed for Embodied AI and Humanoid Robots</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Humanoid</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Loco-manipulation</code> </div> <div class="author"> <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu</a>, Qi Xie, <em>Zeyu Zhang</em>, Tao Yuan, Xiaokun Leng, Lining Sun, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://jingwen-zhang-aaron.github.io/" rel="external nofollow noopener" target="_blank">Jingwen Zhang<sup>†</sup></a>, Zhicheng He<sup>†</sup>, and <a href="https://yaosu.info/" rel="external nofollow noopener" target="_blank">Yao Su<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† denotes corresponding author"> </i> </div> <div class="periodical"> <em>Journal of Field Robotics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1002/rob.22588" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2409.01559" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=O0xRILrc6nk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/pr2-humanoid/PR2-Platform" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://pr2-humanoid.github.io/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>This paper presents the development of a Physics-realistic and Photo-realistic humanoid robot testbed, PR2, to facilitate collaborative research between Embodied Artificial Intelligence (Embodied AI) and robotics. PR2 offers high-quality scene rendering and robot dynamic simulation, enabling (i) the creation of diverse scenes using various digital assets, (ii) the integration of advanced perception or foundation models, and (iii) the implementation of planning and control algorithms for dynamic humanoid robot behaviors based on environmental feedback. The beta version of PR2 has been deployed for the simulation track of a nationwide full-size humanoid robot competition for college students, attracting 137 teams and over 400 participants within four months. This competition covered traditional tasks in bipedal walking, as well as novel challenges in loco-manipulation and language-instruction-based object search, marking a first for public college robotics competitions. A retrospective analysis of the competition suggests that future events should emphasize the integration of locomotion with manipulation and perception. By making the PR2 testbed publicly, we aim to further advance education and training in humanoid robotics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024pr2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PR2: A Physics-and Photo-realistic Testbed for Embodied AI and Humanoid Robots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Hangxin and Xie, Qi and Zhang, Zeyu and Yuan, Tao and Leng, Xiaokun and Sun, Lining and Zhu, Song-Chun and Zhang, Jingwen and He, Zhicheng and Su, Yao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Field Robotics}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1002/rob.22588}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://www.sciencedirect.com/journal/engineering" rel="external nofollow noopener" target="_blank">Engineering</a> </abbr> <figure> <picture> <img src="/papers/2024-engr-glove/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-engr-glove/thumbnail.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024reconfigurable" class="col-sm-8"> <div class="title">A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tool</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tactile</code> </div> <div class="author"> <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*†</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <a href="https://www.zlz.link/" rel="external nofollow noopener" target="_blank">Zhenliang Zhang</a>, <a href="https://www.cs.cmu.edu/~minchenl/" rel="external nofollow noopener" target="_blank">Minchen Li</a>, <a href="https://www.math.ucla.edu/~cffjiang/" rel="external nofollow noopener" target="_blank">Chenfanfu Jiang</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>Engineering</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2024-engr-glove/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/686146591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/engr-glove" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object’s physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system’s three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024reconfigurable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Hangxin and Zhang, Zeyu and Jiao, Ziyuan and Zhang, Zhenliang and Li, Minchen and Jiang, Chenfanfu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{202--216}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e83e8c"> <a href="https://link.springer.com/journal/10055" rel="external nofollow noopener" target="_blank">VR</a> </abbr> <figure> <picture> <img src="/papers/2024-vr-sr/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-vr-sr/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2024emergence" class="col-sm-8"> <div class="title">On the Emergence of Symmetrical Reality</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">VR</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">AR</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> </div> <div class="author"> <a href="https://www.zlz.link/" rel="external nofollow noopener" target="_blank">Zhenliang Zhang</a>, <em>Zeyu Zhang</em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://yaosu.info/" rel="external nofollow noopener" target="_blank">Yao Su</a>, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu</a>, <a href="https://cognn.com/" rel="external nofollow noopener" target="_blank">Wei Wang</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> </div> <div class="periodical"> <em>In IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2401.15132.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/cFj8GL70d7k" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/ieeevr2024sr" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Artificial intelligence (AI) has revolutionized human cognitive abilities and facilitated the development of new AI entities capable of interacting with humans in both physical and virtual environments. Despite the existence of virtual reality, mixed reality, and augmented reality for several years, integrating these technical fields remains a formidable challenge due to their disparate application directions. The advent of AI agents, capable of autonomous perception and action, further compounds this issue by exposing the limitations of traditional human-centered research approaches. It is imperative to establish a comprehensive framework that accommodates the dual perceptual centers of humans and AI agents in both physical and virtual worlds. In this paper, we introduce the symmetrical reality framework, which offers a unified representation encompassing various forms of physical-virtual amalgamations. This framework enables researchers to better comprehend how AI agents can collaborate with humans and how distinct technical pathways of physical-virtual integration can be consolidated from a broader perspective. We then delve into the coexistence of humans and AI, demonstrating a prototype system that exemplifies the operation of symmetrical reality systems for specific tasks, such as pouring water. Subsequently, we propose an instance of an AI-driven active assistance service that illustrates the potential applications of symmetrical reality. This paper aims to offer beneficial perspectives and guidance for researchers and practitioners in different fields, thus contributing to the ongoing research about human-AI coexistence in both physical and virtual environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024emergence</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Emergence of Symmetrical Reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zhenliang and Zhang, Zeyu and Jiao, Ziyuan and Su, Yao and Liu, Hangxin and Wang, Wei and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Conference Virtual Reality and 3D User Interfaces (VR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{639--649}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2024-iros-uav/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-iros-uav/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="su2024flight" class="col-sm-8"> <div class="title">Flight Structure Optimization of Modular Reconfigurable UAVs</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Optimization</code> </div> <div class="author"> <a href="https://yaosu.info/" rel="external nofollow noopener" target="_blank">Yao Su</a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <em>Zeyu Zhang</em>, <a href="https://jingwen-zhang-aaron.github.io/" rel="external nofollow noopener" target="_blank">Jingwen Zhang</a>, Hang Li, <a href="https://wangmeng13thu.github.io/" rel="external nofollow noopener" target="_blank">Meng Wang</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2407.03724" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents a Genetic Algorithm (GA) designed to reconfigure a large group of modular Unmanned Aerial Vehicles (UAVs), each with different weights and inertia parameters, into an over-actuated flight structure with improved dynamic properties. Previous research efforts either utilized expert knowledge to design flight structures for a specific task or relied on enumeration-based algorithms that required extensive computation to find an optimal one. However, both approaches encounter challenges in accommodating the heterogeneity among modules. Our GA addresses these challenges by incorporating the complexities of over-actuation and dynamic properties into its formulation. Additionally, we employ a tree representation and a vector representation to describe flight structures, facilitating efficient crossover operations and fitness evaluations within the GA framework, respectively. Using cubic modular quadcopters capable of functioning as omni-directional thrust generators, we validate that the proposed approach can (i) adeptly identify suboptimal configurations ensuring over-actuation while ensuring trajectory tracking accuracy and (ii) significantly reduce computational costs compared to traditional enumeration-based methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">su2024flight</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flight Structure Optimization of Modular Reconfigurable UAVs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Su, Yao and Jiao, Ziyuan and Zhang, Zeyu and Zhang, Jingwen and Li, Hang and Wang, Meng and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4556--4562}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2024-iros-llm3/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-iros-llm3/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024llm3" class="col-sm-8"> <div class="title">LLM3: Large Language Model-based Task and Motion Planning with Motion Failure Reasoning</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">LLM</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> </div> <div class="author"> Shu Wang<sup>*</sup>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*†</sup></a>, <em>Zeyu Zhang</em>, Ying Nian Wu, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2403.11552" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/AssassinWS/LLM-TAMP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/AssassinWS/LLM-TAMP" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feedback through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain-specific messages between them. Through a series of simulations in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP problems and the efficiency in selecting action parameters. Ablation studies underscore the significant contribution of motion failure reasoning to the success of LLM^3. Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2024llm3</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLM3: Large Language Model-based Task and Motion Planning with Motion Failure Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Shu and Han, Muzhi and Jiao, Ziyuan and Zhang, Zeyu and Wu, Ying Nian and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12086--12092}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2023-iros-part-scene/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iros-part-scene/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023part" class="col-sm-8"> <div class="title">Part-level Scene Reconstruction Affords Robot Interaction</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Affordance</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, Lexing Zhang<sup>*</sup>, Zaijin Wang, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2023-iros-part-scene/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/849400128" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/iros-part-scene" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Existing methods for reconstructing interactive scenes primarily focus on replacing reconstructed objects with CAD models retrieved from a limited database, resulting in significant discrepancies between the reconstructed and observed scenes. To address this issue, our work introduces a part-level reconstruction approach that reassembles objects using primitive shapes. This enables us to precisely replicate the observed physical scenes and simulate robot interactions with both rigid and articulated objects. By segmenting reconstructed objects into semantic parts and aligning primitive shapes to these parts, we assemble them as CAD models while estimating kinematic relations, including parent-child contact relations, joint types, and parameters. Specifically, we derive the optimal primitive alignment by solving a series of optimization problems, and estimate kinematic relations based on part semantics and geometry. Our experiments demonstrate that part-level scene reconstruction outperforms object-level reconstruction by accurately capturing finer details and improving precision. These reconstructed part-level interactive scenes provide valuable kinematic information for various robotic applications; we showcase the feasibility of certifying mobile manipulation planning in these interactive scenes before executing tasks in the physical world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023part</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Part-level Scene Reconstruction Affords Robot Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Zhang, Lexing and Wang, Zaijin and Jiao, Ziyuan and Han, Muzhi and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11178--11185}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2023-iros-object-cutting/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iros-object-cutting/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023learning" class="col-sm-8"> <div class="title">Learning a Causal Transition Model for Object Cutting</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Skill Learning</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2023-iros-object-cutting/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/851269542" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/iros-object-cutting" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Cutting objects into desired fragments is challenging for robots due to the spatially unstructured nature of fragments and the complex one-to-many object fragmentation caused by actions. We present a novel approach to model object fragmentation using an attributed stochastic grammar. This grammar abstracts fragment states as node variables and captures causal transitions in object fragmentation through production rules. We devise a probabilistic framework to learn this grammar from human demonstrations. The planning process for object cutting involves inferring an optimal parse tree of desired fragments using the learned grammar, with parse tree productions corresponding to cutting actions. We employ Monte Carlo Tree Search (MCTS) to efficiently approximate the optimal parse tree and generate a sequence of executable cutting actions. The experiments demonstrate the efficacy in planning for object-cutting tasks, both in simulation and on a physical robot. The proposed approach outperforms several baselines by demonstrating superior generalization to novel setups, thanks to the compositionality of the grammar model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning a Causal Transition Model for Object Cutting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Han, Muzhi and Jia, Baoxiong and Jiao, Ziyuan and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1996--2003}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e83e8c"> <a href="https://iccv.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV</a> </abbr> <figure> <picture> <img src="/papers/2023-iccv-voe/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iccv-voe/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dai2023x" class="col-sm-8"> <div class="title">X-VoE: Measuring Explanatory Violation of Expectation in Physical Events</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Physics</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">CogSci</code> </div> <div class="author"> <a href="https://daibopku.github.io/daibo/" rel="external nofollow noopener" target="_blank">Bo Dai</a>, Linge Wang, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a>, <em>Zeyu Zhang</em>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://wellyzhang.github.io/" rel="external nofollow noopener" target="_blank">Chi Zhang<sup>†</sup></a>, and <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In In Proceedings of the International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2308.10441" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://vimeo.com/855745233" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/daibopku/X-VoE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/x-voe" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Intuitive physics is pivotal for human understanding of the physical world, enabling prediction and interpretation of events even in infancy. Nonetheless, replicating this level of intuitive physics in artificial intelligence (AI) remains a formidable challenge. This study introduces X-VoE, a comprehensive benchmark dataset, to assess AI agents’ grasp of intuitive physics. Built on the developmental psychology-rooted Violation of Expectation (VoE) paradigm, X-VoE establishes a higher bar for the explanatory capacities of intuitive physics models. Each VoE scenario within X-VoE encompasses three distinct settings, probing models’ comprehension of events and their underlying explanations. Beyond model evaluation, we present an explanation-based learning system that captures physics dynamics and infers occluded object states solely from visual sequences, without explicit occlusion labels. Experimental outcomes highlight our model’s alignment with human commonsense when tested against X-VoE. A remarkable feature is our model’s ability to visually expound VoE events by reconstructing concealed scenes. Concluding, we discuss the findings’ implications and outline future research directions. Through X-VoE, we catalyze the advancement of AI endowed with human-like intuitive physics capabilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dai2023x</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-VoE: Measuring Explanatory Violation of Expectation in Physical Events}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dai, Bo and Wang, Linge and Jia, Baoxiong and Zhang, Zeyu and Zhu, Song-Chun and Zhang, Chi and Zhu, Yixin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{In Proceedings of the International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3992--4002}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2022-iros-ged/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-iros-ged/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2022sequential" class="col-sm-8"> <div class="title">Sequential Manipulation Planning on Scene Graph</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> </div> <div class="author"> <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, Yida Niu, <em>Zeyu Zhang</em>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-iros-ged/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/728421779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/zyjiao4728/POG-Demo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/planning-on-graph" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We devise a 3D scene graph representation, contact graph+ (cg+), for efficient sequential task planning. Augmented with predicate-like attributes, this contact graph-based representation abstracts scene layouts with succinct geometric information and valid robot-scene interactions. Goal configurations, naturally specified on contact graphs, can be produced by a genetic algorithm with a stochastic optimization method. A task plan is then initialized by computing the Graph Editing Distance (GED) between the initial contact graphs and the goal configurations, which generates graph edit operations corresponding to possible robot actions. We finalize the task plan by imposing constraints to regulate the temporal feasibility of graph edit operations, ensuring valid task and motion correspondences. In a series of simulations and experiments, robots successfully complete complex sequential object rearrangement tasks that are difficult to specify using conventional planning language like Planning Domain Definition Language (PDDL), demonstrating the high feasibility and potential of robot sequential task planning on contact graph.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiao2022sequential</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sequential Manipulation Planning on Scene Graph}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiao, Ziyuan and Niu, Yida and Zhang, Zeyu and Zhu, Song-Chun and Zhu, Yixin and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8203--8210}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6610f2"> <a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank">RA-L</a> </abbr> <figure> <picture> <img src="/papers/2022-ral-tooluse/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-ral-tooluse/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2022understanding" class="col-sm-8"> <div class="title">Understanding Physical Effects for Effective Tool-use</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tool</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Affordance</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">HOI</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, Weiqi Wang, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-ral-tooluse/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/725191188" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/ral-tooluse" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2022understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding Physical Effects for Effective Tool-use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Jiao, Ziyuan and Wang, Weiqi and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9469--9476}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ffc107"> <a href="https://link.springer.com/journal/11263" rel="external nofollow noopener" target="_blank">IJCV</a> </abbr> <figure> <picture> <img src="/papers/2022-ijcv-scene/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-ijcv-scene/thumbnail.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="han2022scene" class="col-sm-8"> <div class="title">Scene Reconstruction with Functional Objects for Robot Autonomy</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://xuxie1031.github.io/" rel="external nofollow noopener" target="_blank">Xu Xie</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>International Journal of Computer Vision (IJCV)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-ijcv-scene/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://drive.google.com/file/d/1yYeIXvpWshZPjwDuouOPeQ4Xr6TX7-C2/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/hmz-15/Interactive-Scene-Reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/ijcv2022-reconstruction" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">han2022scene</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scene Reconstruction with Functional Objects for Robot Autonomy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Vision (IJCV)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{130}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2940--2961}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2021-icra-scene-recon/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-icra-scene-recon/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="han2021reconstructing" class="col-sm-8"> <div class="title">Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://xuxie1031.github.io/" rel="external nofollow noopener" target="_blank">Xu Xie</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2021-icra-scene-recon/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/530222887" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/hmz-15/Interactive-Scene-Reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/icra2021-reconstruction/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>In this paper, we rethink the problem of scene reconstruction from an embodied agent’s perspective: While the classic view focuses on the reconstruction accuracy, our new perspective emphasizes the underlying functions and constraints such that the reconstructed scenes provide actionable information for simulating interactions with agents. Here, we address this challenging problem by reconstructing an interactive scene using RGB-D data stream, which captures (i) the semantics and geometry of objects and layouts by a 3D volumetric panoptic mapping module, and (ii) object affordance and contextual relations by reasoning over physical common sense among objects, organized by a graph-based scene representation. Crucially, this reconstructed scene replaces the object meshes in the dense panoptic map with part-based articulated CAD models for finer-grained robot interactions. In the experiments, we demonstrate that (i) our panoptic mapping module outperforms previous state-of-the-art methods, (ii) a high-performant physical reasoning procedure that matches, aligns, and replaces objects’ meshes with best-fitted CAD models, and (iii) reconstructed scenes are physically plausible and naturally afford actionable interactions; without any manual labeling, they are seamlessly imported to ROS-based simulators and virtual environments for complex robot task executions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">han2021reconstructing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12199--12206}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2021-iros-vkc-motion/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-iros-vkc-motion/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2021consolidating" class="col-sm-8"> <div class="title">Consolidating Kinematic Models to Promote Coordinated Mobile Manipulations</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> </div> <div class="author"> <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://jiangxjames.github.io/" rel="external nofollow noopener" target="_blank">Xin Jiang</a>, David Han, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2021-iros-vkc-motion/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/581565256" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/zyjiao4728/VKC-Demo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/iros2021-vkc/home/vkc-motion" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We construct a Virtual Kinematic Chain (VKC) that readily consolidates the kinematics of the mobile base, the arm, and the object to be manipulated in mobile manipulations. Accordingly, a mobile manipulation task is represented by altering the state of the constructed VKC, which can be converted to a motion planning problem, formulated, and solved by trajectory optimization. This new VKC perspective of mobile manipulation allows a service robot to (i) produce well-coordinated motions, suitable for complex household environments, and (ii) perform intricate multi-step tasks while interacting with multiple objects without an explicit definition of intermediate goals. In simulated experiments, we validate these advantages by comparing the VKC-based approach with baselines that solely optimize individual components. The results manifest that VKC-based joint modeling and planning promote task success rates and produce more efficient trajectories.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiao2021consolidating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Consolidating Kinematic Models to Promote Coordinated Mobile Manipulations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiao, Ziyuan and Zhang, Zeyu and Jiang, Xin and Han, David and Zhu, Song-Chun and Zhu, Yixin and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{979--985}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2021-iros-vkc-task/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-iros-vkc-task/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2021efficient" class="col-sm-8"> <div class="title">Efficient Task Planning for Mobile Manipulation: a Virtual Kinematic Chain Perspective</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> </div> <div class="author"> <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, Weiqi Wang, David Han, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and Hangxin† Liu <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2021-iros-vkc-task/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/581563536" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/TooSchoolForCool/IROS21-VKC-Task-PDDL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/papers/iros-vkc-task" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We present a Virtual Kinematic Chain (VKC) perspective, a simple yet effective method, to improve task planning efficacy for mobile manipulation. By consolidating the kinematics of the mobile base, the arm, and the object to be manipulated collectively as a whole, our novel VKC perspective naturally defines abstract actions and eliminates unnecessary predicates in describing intermediate poses. As a result, these advantages simplify the design of the planning domain and significantly reduce the search space and branching factors in solving planning problems. In experiments, we implement a task planner using Planning Domain Definition Language (PDDL) with VKC. Compared with classic domain definition, our VKC-based domain definition is more efficient in both planning time and memory required. In addition, the abstract actions perform better in producing feasible motion plans and trajectories. We further scale up the VKC-based task planner in complex mobile manipulation tasks. Taken together, these results demonstrate that task planning using VKC for mobile manipulation is not only natural and effective but also introduces new capabilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiao2021efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Task Planning for Mobile Manipulation: a Virtual Kinematic Chain Perspective}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiao, Ziyuan and Zhang, Zeyu and Wang, Weiqi and Han, David and Zhu, Song-Chun and Zhu, Yixin and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8288--8294}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2020-icra-ar-evacuation/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-icra-ar-evacuation/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2020congestion" class="col-sm-8"> <div class="title">Congestion-aware Evacuation Routing using Augmented Reality Devices</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">AR</code> </div> <div class="author"> <em>Zeyu Zhang</em>, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu</a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2020-icra-ar-evacuation/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/391765127" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/TooSchoolForCool/AR-Evacuation-ICRA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees’ locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end AR devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020congestion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Congestion-aware Evacuation Routing using Augmented Reality Devices}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Liu, Hangxin and Jiao, Ziyuan and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2798--2804}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2020-icra-ar-workspace/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-icra-ar-workspace/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiu2020human" class="col-sm-8"> <div class="title">Human-robot Interaction in a Shared Augmented Reality Workspace</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">AR</code> </div> <div class="author"> <a href="https://janetalready.github.io/" rel="external nofollow noopener" target="_blank">Shuwen Qiu<sup>*</sup></a>, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*</sup></a>, <em>Zeyu Zhang</em>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2020-icra-ar-workspace/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/439150333" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Janetalready/Shared-AR-workspace" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/shared-ar-workspace/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We design and develop a new shared Augmented Reality (AR) workspace for Human-Robot Interaction (HRI), which establishes a bi-directional communication between human agents and robots. In a prototype system, the shared AR workspace enables a shared perception, so that a physical robot not only perceives the virtual elements in its own view but also infers the utility of the human agent—the cost needed to perceive and interact in AR—by sensing the human agent’s gaze and pose. Such a new HRI design also affords a shared manipulation, wherein the physical robot can control and alter virtual objects in AR as an active agent; crucially, a robot can proactively interact with human agents, instead of purely passively executing received commands. In experiments, we design a resource collection game that qualitatively demonstrates how a robot perceives, processes, and manipulates in AR and quantitatively evaluates the efficacy of HRI using the shared AR workspace. We further discuss how the system can potentially benefit future HRI studies that are otherwise challenging.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">qiu2020human</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-robot Interaction in a Shared Augmented Reality Workspace}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qiu, Shuwen and Liu, Hangxin and Zhang, Zeyu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11413--11418}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2019-icra-ssl/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2019-icra-ssl/thumbnail.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2019self" class="col-sm-8"> <div class="title">Self-supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> </div> <div class="author"> <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors"> </i> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2019-icra-ssl/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/321150838" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/TooSchoolForCool/EddieBot-ROS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper presents an incremental learning framework for mobile robots localizing the human sound source using a microphone array in a complex indoor environment consisting of multiple rooms. In contrast to conventional approaches that leverage direction-of-arrival estimation, the framework allows a robot to accumulate training data and improve the performance of the prediction model over time using an incremental learning scheme. Specifically, we use implicit acoustic features obtained from an auto-encoder together with the geometry features from the map for training. A self-supervision process is developed such that the model ranks the priority of rooms to explore and assigns the ground truth label to the collected data, updating the learned model on-the-fly. The framework does not require pre-collected data and can be directly applied to real-world scenarios without any human supervisions or interventions. In experiments, we demonstrate that the prediction accuracy reaches 67% using about 20 training samples and eventually achieves 90% accuracy within 120 samples, surpassing prior classification-based methods with explicit GCC-PHAT features.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2019self</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Hangxin and Zhang, Zeyu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2599--2605}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zeyu Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>