<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zeyu Zhang | BIGAI </title> <meta name="author" content="Zeyu Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="Cognitive Robot, Manipulation, Robot Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tooschoolforcool.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%7A%65%79%75%7A%68%61%6E%67@%75%63%6C%61.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/tooschoolforcool" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://scholar.google.com/citations?user=YhQCNUkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Zeyu Zhang | BIGAI </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/authors/zyzhang_full-1600.webp 1600w,/assets/img/authors/zyzhang_full-3600.webp 3600w," type="image/webp" sizes="(min-width: 950px) 276.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/authors/zyzhang_full.jpg?bfaaff8c59336491ddcaab3c368576f0" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="zyzhang_full.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>zeyuzhang AT ucla.edu</p> <p>BIGAI, Beijing, China</p> </div> </div> <div class="clearfix"> <p>I am a research scientist in the National Key Laboratory of General Artificial Intelligence at <a href="https://www.bigai.ai/" rel="external nofollow noopener" target="_blank">Beijing Institute for General Artificial Intelligence (BIGAI)</a>, working on building intelligent robot system that can understand and interact with the world.</p> <p>I received a Ph.D. degree in Computer Science from UCLA in 2022, where I was a member of the <a href="https://communitypartnerships.ucla.edu/program/center-vision-cognition-learning-and-autonomy-vcla/" rel="external nofollow noopener" target="_blank">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>, advised by <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Prof. Song-Chun Zhu</a>. My work at UCLA was supported by DARPA XAI, ONR MURI, and ONR Cognitive Robot.</p> <p>Before coming to UCLA, I graduated with a B.S. in Computer Science from Hunan University in 2017.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 20, 2025</th> <td> One paper have been accepted by IEEE Transactions on Consumer Electronics (TCE). </td> </tr> <tr> <th scope="row" style="width: 20%">May 20, 2025</th> <td> One paper have been accepted by RAL. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 25, 2025</th> <td> One paper have been accepted by Journal of Field Robotics. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 03, 2025</th> <td> One paper have been accepted by TPAMI. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 28, 2024</th> <td> Two papers have been accepted by IROS 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 15, 2024</th> <td> One paper have been accepted by VR 2024. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6610f2"> <a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank">RA-L</a> </abbr> <figure> <picture> <img src="/papers/2025-ral-m3bench/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-ral-m3bench/thumbnail.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025m3bench" class="col-sm-8"> <div class="title">M3Bench: Benchmarking Whole-Body Motion Generation for Mobile Manipulation in 3D Scenes</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Mobile Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Motion Generation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Benchmark</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sixu-yan.github.io/" rel="external nofollow noopener" target="_blank">Sixu Yan<sup>*</sup></a>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, Zaijin Wang, <a href="https://xwcv.github.io/" rel="external nofollow noopener" target="_blank">Xinggang Wang</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.06678" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/TwJQnRm663M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="#" class="btn btn-sm z-depth-0" role="button">Code</a> <a href="/papers/m3bench" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We propose M3Bench, a new benchmark for whole-body motion generation in mobile manipulation tasks. Given a 3D scene context, M3Bench requires an embodied agent to reason about its configuration, environmental constraints, and task objectives to generate coordinated whole-body motion trajectories for object rearrangement. M3Bench features 30,000 object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly developed M3BenchMaker, an automatic data generation tool that produces whole-body motion trajectories from high-level task instructions using only basic scene and robot information. Our benchmark includes various task splits to evaluate generalization across different dimensions and leverages realistic physics simulation for trajectory assessment. Extensive evaluation analysis reveals that state-of-the-art models struggle with coordinating base-arm motion while adhering to environmental and task-specific constraints, underscoring the need for new models to bridge this gap. By releasing M3Bench and M3BenchMaker at https://zeyuzhang.com/papers/m3bench, we aim to advance robotics research toward more adaptive and capable mobile manipulation in diverse, real-world environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2025m3bench</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{M3Bench: Benchmarking Whole-Body Motion Generation for Mobile Manipulation in 3D Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Yan, Sixu and Han, Muzhi and Wang, Zaijin and Wang, Xinggang and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#fd7e14"> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" rel="external nofollow noopener" target="_blank">T-PAMI</a> </abbr> <figure> <picture> <img src="/papers/2025-tpami-m2diffuser/thumbnail.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2025-tpami-m2diffuser/thumbnail.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yan2025m2diffuser" class="col-sm-8"> <div class="title">M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Mobile Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Motion Generation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Trajectory Optimization</code> </div> <div class="author"> <a href="https://sixu-yan.github.io/" rel="external nofollow noopener" target="_blank">Sixu Yan</a>, <em>Zeyu Zhang</em>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, Zaijin Wang, Qi Xie, Zhitian Li, Zhehan Li, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a>, <a href="https://xwcv.github.io/" rel="external nofollow noopener" target="_blank">Xinggang Wang<sup>†</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2410.11402" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/T7kpDifRtfk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/m2diffuser/M2Diffuser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://m2diffuser.github.io/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yan2025m2diffuser</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Sixu and Zhang, Zeyu and Han, Muzhi and Wang, Zaijin and Xie, Qi and Li, Zhitian and Li, Zhehan and Liu, Hangxin and Wang, Xinggang and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://www.sciencedirect.com/journal/engineering" rel="external nofollow noopener" target="_blank">Engineering</a> </abbr> <figure> <picture> <img src="/papers/2024-engr-glove/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-engr-glove/thumbnail.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024reconfigurable" class="col-sm-8"> <div class="title">A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tool</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tactile</code> </div> <div class="author"> <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*†</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <a href="https://www.zlz.link/" rel="external nofollow noopener" target="_blank">Zhenliang Zhang</a>, <a href="https://www.cs.cmu.edu/~minchenl/" rel="external nofollow noopener" target="_blank">Minchen Li</a>, <a href="https://www.math.ucla.edu/~cffjiang/" rel="external nofollow noopener" target="_blank">Chenfanfu Jiang</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>Engineering</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2024-engr-glove/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/686146591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/engr-glove" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object’s physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system’s three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024reconfigurable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Hangxin and Zhang, Zeyu and Jiao, Ziyuan and Zhang, Zhenliang and Li, Minchen and Jiang, Chenfanfu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{202--216}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2023-iros-object-cutting/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iros-object-cutting/thumbnail.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023learning" class="col-sm-8"> <div class="title">Learning a Causal Transition Model for Object Cutting</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Skill Learning</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2023-iros-object-cutting/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/851269542" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/iros-object-cutting" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Cutting objects into desired fragments is challenging for robots due to the spatially unstructured nature of fragments and the complex one-to-many object fragmentation caused by actions. We present a novel approach to model object fragmentation using an attributed stochastic grammar. This grammar abstracts fragment states as node variables and captures causal transitions in object fragmentation through production rules. We devise a probabilistic framework to learn this grammar from human demonstrations. The planning process for object cutting involves inferring an optimal parse tree of desired fragments using the learned grammar, with parse tree productions corresponding to cutting actions. We employ Monte Carlo Tree Search (MCTS) to efficiently approximate the optimal parse tree and generate a sequence of executable cutting actions. The experiments demonstrate the efficacy in planning for object-cutting tasks, both in simulation and on a physical robot. The proposed approach outperforms several baselines by demonstrating superior generalization to novel setups, thanks to the compositionality of the grammar model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning a Causal Transition Model for Object Cutting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Han, Muzhi and Jia, Baoxiong and Jiao, Ziyuan and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1996--2003}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6610f2"> <a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank">RA-L</a> </abbr> <figure> <picture> <img src="/papers/2022-ral-tooluse/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-ral-tooluse/thumbnail.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2022understanding" class="col-sm-8"> <div class="title">Understanding Physical Effects for Effective Tool-use</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tool</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Affordance</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">HOI</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, Weiqi Wang, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-ral-tooluse/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/725191188" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/ral-tooluse" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2022understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding Physical Effects for Effective Tool-use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Jiao, Ziyuan and Wang, Weiqi and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9469--9476}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ffc107"> <a href="https://link.springer.com/journal/11263" rel="external nofollow noopener" target="_blank">IJCV</a> </abbr> <figure> <picture> <img src="/papers/2022-ijcv-scene/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-ijcv-scene/thumbnail.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="han2022scene" class="col-sm-8"> <div class="title">Scene Reconstruction with Functional Objects for Robot Autonomy</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://xuxie1031.github.io/" rel="external nofollow noopener" target="_blank">Xu Xie</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>International Journal of Computer Vision (IJCV)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-ijcv-scene/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://drive.google.com/file/d/1yYeIXvpWshZPjwDuouOPeQ4Xr6TX7-C2/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/hmz-15/Interactive-Scene-Reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/ijcv2022-reconstruction" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">han2022scene</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scene Reconstruction with Functional Objects for Robot Autonomy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Vision (IJCV)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{130}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2940--2961}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%7A%65%79%75%7A%68%61%6E%67@%75%63%6C%61.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/tooschoolforcool" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://scholar.google.com/citations?user=YhQCNUkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note">If you have any questions, please feel free to contact me via email. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zeyu Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>