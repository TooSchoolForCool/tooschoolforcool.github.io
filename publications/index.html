<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Zeyu Zhang | BIGAI </title> <meta name="author" content="Zeyu Zhang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="Cognitive Robot, Manipulation, Robot Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tooschoolforcool.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zeyu Zhang | BIGAI </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <a href="https://www.sciencedirect.com/journal/engineering" rel="external nofollow noopener" target="_blank">Engineering</a> </abbr> <figure> <picture> <img src="/papers/2024-engr-glove/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-engr-glove/thumbnail.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024reconfigurable" class="col-sm-8"> <div class="title">A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tool</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tactile</code> </div> <div class="author"> <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*†</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <a href="https://www.zlz.link/" rel="external nofollow noopener" target="_blank">Zhenliang Zhang</a>, <a href="https://www.cs.cmu.edu/~minchenl/" rel="external nofollow noopener" target="_blank">Minchen Li</a>, <a href="https://www.math.ucla.edu/~cffjiang/" rel="external nofollow noopener" target="_blank">Chenfanfu Jiang</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>Engineering</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2024-engr-glove/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/686146591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://sites.google.com/view/engr-glove" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object’s physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system’s three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024reconfigurable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Hangxin and Zhang, Zeyu and Jiao, Ziyuan and Zhang, Zhenliang and Li, Minchen and Jiang, Chenfanfu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{202--216}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2023-iros-part-scene/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iros-part-scene/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023part" class="col-sm-8"> <div class="title">Part-level Scene Reconstruction Affords Robot Interaction</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Affordance</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, Lexing Zhang<sup>*</sup>, Zaijin Wang, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2023-iros-part-scene/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/849400128" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/iros-part-scene" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Existing methods for reconstructing interactive scenes primarily focus on replacing reconstructed objects with CAD models retrieved from a limited database, resulting in significant discrepancies between the reconstructed and observed scenes. To address this issue, our work introduces a part-level reconstruction approach that reassembles objects using primitive shapes. This enables us to precisely replicate the observed physical scenes and simulate robot interactions with both rigid and articulated objects. By segmenting reconstructed objects into semantic parts and aligning primitive shapes to these parts, we assemble them as CAD models while estimating kinematic relations, including parent-child contact relations, joint types, and parameters. Specifically, we derive the optimal primitive alignment by solving a series of optimization problems, and estimate kinematic relations based on part semantics and geometry. Our experiments demonstrate that part-level scene reconstruction outperforms object-level reconstruction by accurately capturing finer details and improving precision. These reconstructed part-level interactive scenes provide valuable kinematic information for various robotic applications; we showcase the feasibility of certifying mobile manipulation planning in these interactive scenes before executing tasks in the physical world.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023part</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Part-level Scene Reconstruction Affords Robot Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Zhang, Lexing and Wang, Zaijin and Jiao, Ziyuan and Han, Muzhi and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11178--11185}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2023-iros-object-cutting/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iros-object-cutting/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023learning" class="col-sm-8"> <div class="title">Learning a Causal Transition Model for Object Cutting</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Skill Learning</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2023-iros-object-cutting/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/851269542" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/iros-object-cutting" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Cutting objects into desired fragments is challenging for robots due to the spatially unstructured nature of fragments and the complex one-to-many object fragmentation caused by actions. We present a novel approach to model object fragmentation using an attributed stochastic grammar. This grammar abstracts fragment states as node variables and captures causal transitions in object fragmentation through production rules. We devise a probabilistic framework to learn this grammar from human demonstrations. The planning process for object cutting involves inferring an optimal parse tree of desired fragments using the learned grammar, with parse tree productions corresponding to cutting actions. We employ Monte Carlo Tree Search (MCTS) to efficiently approximate the optimal parse tree and generate a sequence of executable cutting actions. The experiments demonstrate the efficacy in planning for object-cutting tasks, both in simulation and on a physical robot. The proposed approach outperforms several baselines by demonstrating superior generalization to novel setups, thanks to the compositionality of the grammar model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning a Causal Transition Model for Object Cutting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Han, Muzhi and Jia, Baoxiong and Jiao, Ziyuan and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1996--2003}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e83e8c"> <a href="https://iccv.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV</a> </abbr> <figure> <picture> <img src="/papers/2023-iccv-voe/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-iccv-voe/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dai2023x" class="col-sm-8"> <div class="title">X-VoE: Measuring Explanatory Violation of Expectation in Physical Events</div> <div class="author"> <a href="https://daibopku.github.io/daibo/" rel="external nofollow noopener" target="_blank">Bo Dai</a>, Linge Wang, <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a>, <em>Zeyu Zhang</em>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://wellyzhang.github.io/" rel="external nofollow noopener" target="_blank">Chi Zhang</a>, and <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In In Proceedings of the International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2308.10441" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://vimeo.com/855745233" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/daibopku/X-VoE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/x-voe" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>Intuitive physics is pivotal for human understanding of the physical world, enabling prediction and interpretation of events even in infancy. Nonetheless, replicating this level of intuitive physics in artificial intelligence (AI) remains a formidable challenge. This study introduces X-VoE, a comprehensive benchmark dataset, to assess AI agents’ grasp of intuitive physics. Built on the developmental psychology-rooted Violation of Expectation (VoE) paradigm, X-VoE establishes a higher bar for the explanatory capacities of intuitive physics models. Each VoE scenario within X-VoE encompasses three distinct settings, probing models’ comprehension of events and their underlying explanations. Beyond model evaluation, we present an explanation-based learning system that captures physics dynamics and infers occluded object states solely from visual sequences, without explicit occlusion labels. Experimental outcomes highlight our model’s alignment with human commonsense when tested against X-VoE. A remarkable feature is our model’s ability to visually expound VoE events by reconstructing concealed scenes. Concluding, we discuss the findings’ implications and outline future research directions. Through X-VoE, we catalyze the advancement of AI endowed with human-like intuitive physics capabilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dai2023x</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-VoE: Measuring Explanatory Violation of Expectation in Physical Events}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dai, Bo and Wang, Linge and Jia, Baoxiong and Zhang, Zeyu and Zhu, Song-Chun and Zhang, Chi and Zhu, Yixin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{In Proceedings of the International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3992--4002}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2022-iros-ged/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-iros-ged/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2022sequential" class="col-sm-8"> <div class="title">Sequential Manipulation Planning on Scene Graph</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> </div> <div class="author"> <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, Yida Niu, <em>Zeyu Zhang</em>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-iros-ged/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/728421779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/zyjiao4728/POG-Demo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/planning-on-graph" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We devise a 3D scene graph representation, contact graph+ (cg+), for efficient sequential task planning. Augmented with predicate-like attributes, this contact graph-based representation abstracts scene layouts with succinct geometric information and valid robot-scene interactions. Goal configurations, naturally specified on contact graphs, can be produced by a genetic algorithm with a stochastic optimization method. A task plan is then initialized by computing the Graph Editing Distance (GED) between the initial contact graphs and the goal configurations, which generates graph edit operations corresponding to possible robot actions. We finalize the task plan by imposing constraints to regulate the temporal feasibility of graph edit operations, ensuring valid task and motion correspondences. In a series of simulations and experiments, robots successfully complete complex sequential object rearrangement tasks that are difficult to specify using conventional planning language like Planning Domain Definition Language (PDDL), demonstrating the high feasibility and potential of robot sequential task planning on contact graph.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiao2022sequential</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sequential Manipulation Planning on Scene Graph}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiao, Ziyuan and Niu, Yida and Zhang, Zeyu and Zhu, Song-Chun and Zhu, Yixin and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8203--8210}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#6610f2"> <a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank">RA-L</a> </abbr> <figure> <picture> <img src="/papers/2022-ral-tooluse/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-ral-tooluse/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2022understanding" class="col-sm-8"> <div class="title">Understanding Physical Effects for Effective Tool-use</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Tool</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Affordance</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">HOI</code> </div> <div class="author"> <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, Weiqi Wang, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-ral-tooluse/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/725191188" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="/papers/ral-tooluse" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2022understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding Physical Effects for Effective Tool-use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Jiao, Ziyuan and Wang, Weiqi and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters (RA-L)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9469--9476}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ffc107"> <a href="https://link.springer.com/journal/11263" rel="external nofollow noopener" target="_blank">IJCV</a> </abbr> <figure> <picture> <img src="/papers/2022-ijcv-scene/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2022-ijcv-scene/thumbnail.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="han2022scene" class="col-sm-8"> <div class="title">Scene Reconstruction with Functional Objects for Robot Autonomy</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://xuxie1031.github.io/" rel="external nofollow noopener" target="_blank">Xu Xie</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu<sup>†</sup></a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>International Journal of Computer Vision (IJCV)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2022-ijcv-scene/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://drive.google.com/file/d/1yYeIXvpWshZPjwDuouOPeQ4Xr6TX7-C2/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/hmz-15/Interactive-Scene-Reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/ijcv2022-reconstruction" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We present a robot learning and planning framework that produces an effective tool-use strategy with the least joint efforts, capable of handling objects different from training. Leveraging a Finite Element Method (FEM)-based simulator that reproduces fine-grained, continuous visual and physical effects given observed tool-use events, the essential physical properties contributing to the effects are identified through the proposed Iterative Deepening Symbolic Regression (IDSR) algorithm. We further devise an optimal control-based motion planning scheme to integrate robot- and tool-specific kinematics and dynamics to produce an effective trajectory that enacts the learned properties. In simulation, we demonstrate that the proposed framework can produce more effective tool-use strategies, drastically different from the observed ones in two exemplar tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">han2022scene</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scene Reconstruction with Functional Objects for Robot Autonomy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Vision (IJCV)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{130}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2940--2961}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2021-icra-scene-recon/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-icra-scene-recon/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="han2021reconstructing" class="col-sm-8"> <div class="title">Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Scene Reconstruction</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Functionality</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Digital Twin</code> </div> <div class="author"> <a href="https://sites.google.com/view/muzhihan/home" rel="external nofollow noopener" target="_blank">Muzhi Han<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://xuxie1031.github.io/" rel="external nofollow noopener" target="_blank">Xu Xie</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2021-icra-scene-recon/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/530222887" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/hmz-15/Interactive-Scene-Reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/icra2021-reconstruction/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>In this paper, we rethink the problem of scene reconstruction from an embodied agent’s perspective: While the classic view focuses on the reconstruction accuracy, our new perspective emphasizes the underlying functions and constraints such that the reconstructed scenes provide actionable information for simulating interactions with agents. Here, we address this challenging problem by reconstructing an interactive scene using RGB-D data stream, which captures (i) the semantics and geometry of objects and layouts by a 3D volumetric panoptic mapping module, and (ii) object affordance and contextual relations by reasoning over physical common sense among objects, organized by a graph-based scene representation. Crucially, this reconstructed scene replaces the object meshes in the dense panoptic map with part-based articulated CAD models for finer-grained robot interactions. In the experiments, we demonstrate that (i) our panoptic mapping module outperforms previous state-of-the-art methods, (ii) a high-performant physical reasoning procedure that matches, aligns, and replaces objects’ meshes with best-fitted CAD models, and (iii) reconstructed scenes are physically plausible and naturally afford actionable interactions; without any manual labeling, they are seamlessly imported to ROS-based simulators and virtual environments for complex robot task executions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">han2021reconstructing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12199--12206}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2021-iros-vkc-motion/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-iros-vkc-motion/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2021consolidating" class="col-sm-8"> <div class="title">Consolidating Kinematic Models to Promote Coordinated Mobile Manipulations</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> </div> <div class="author"> <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://jiangxjames.github.io/" rel="external nofollow noopener" target="_blank">Xin Jiang</a>, David Han, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2021-iros-vkc-motion/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/581565256" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/zyjiao4728/VKC-Demo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/iros2021-vkc/home/vkc-motion" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We construct a Virtual Kinematic Chain (VKC) that readily consolidates the kinematics of the mobile base, the arm, and the object to be manipulated in mobile manipulations. Accordingly, a mobile manipulation task is represented by altering the state of the constructed VKC, which can be converted to a motion planning problem, formulated, and solved by trajectory optimization. This new VKC perspective of mobile manipulation allows a service robot to (i) produce well-coordinated motions, suitable for complex household environments, and (ii) perform intricate multi-step tasks while interacting with multiple objects without an explicit definition of intermediate goals. In simulated experiments, we validate these advantages by comparing the VKC-based approach with baselines that solely optimize individual components. The results manifest that VKC-based joint modeling and planning promote task success rates and produce more efficient trajectories.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiao2021consolidating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Consolidating Kinematic Models to Promote Coordinated Mobile Manipulations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiao, Ziyuan and Zhang, Zeyu and Jiang, Xin and Han, David and Zhu, Song-Chun and Zhu, Yixin and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{979--985}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2021-iros-vkc-task/thumbnail.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2021-iros-vkc-task/thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiao2021efficient" class="col-sm-8"> <div class="title">Efficient Task Planning for Mobile Manipulation: a Virtual Kinematic Chain Perspective</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Manipulation</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">TAMP</code> </div> <div class="author"> <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, Weiqi Wang, David Han, <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and Hangxin† Liu <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors&lt;br&gt;† denotes corresponding author"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2021-iros-vkc-task/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/581563536" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/TooSchoolForCool/IROS21-VKC-Task-PDDL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/papers/iros-vkc-task" target="_blank" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We present a Virtual Kinematic Chain (VKC) perspective, a simple yet effective method, to improve task planning efficacy for mobile manipulation. By consolidating the kinematics of the mobile base, the arm, and the object to be manipulated collectively as a whole, our novel VKC perspective naturally defines abstract actions and eliminates unnecessary predicates in describing intermediate poses. As a result, these advantages simplify the design of the planning domain and significantly reduce the search space and branching factors in solving planning problems. In experiments, we implement a task planner using Planning Domain Definition Language (PDDL) with VKC. Compared with classic domain definition, our VKC-based domain definition is more efficient in both planning time and memory required. In addition, the abstract actions perform better in producing feasible motion plans and trajectories. We further scale up the VKC-based task planner in complex mobile manipulation tasks. Taken together, these results demonstrate that task planning using VKC for mobile manipulation is not only natural and effective but also introduces new capabilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiao2021efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Task Planning for Mobile Manipulation: a Virtual Kinematic Chain Perspective}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiao, Ziyuan and Zhang, Zeyu and Wang, Weiqi and Han, David and Zhu, Song-Chun and Zhu, Yixin and Liu, Hangxin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8288--8294}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2020-icra-ar-evacuation/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-icra-ar-evacuation/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2020congestion" class="col-sm-8"> <div class="title">Congestion-aware Evacuation Routing using Augmented Reality Devices</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">AR</code> </div> <div class="author"> <em>Zeyu Zhang</em>, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu</a>, <a href="https://sites.google.com/g.ucla.edu/zyjiao/home" rel="external nofollow noopener" target="_blank">Ziyuan Jiao</a>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2020-icra-ar-evacuation/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/391765127" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/TooSchoolForCool/AR-Evacuation-ICRA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees’ locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end AR devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2020congestion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Congestion-aware Evacuation Routing using Augmented Reality Devices}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Zeyu and Liu, Hangxin and Jiao, Ziyuan and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2798--2804}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ff5733"> <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" rel="external nofollow noopener" target="_blank">IROS</a> </abbr> <figure> <picture> <img src="/papers/2020-icra-ar-workspace/thumbnail.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2020-icra-ar-workspace/thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiu2020human" class="col-sm-8"> <div class="title">Human-robot Interaction in a Shared Augmented Reality Workspace</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">AR</code> </div> <div class="author"> <a href="https://janetalready.github.io/" rel="external nofollow noopener" target="_blank">Shuwen Qiu<sup>*</sup></a>, <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*</sup></a>, <em>Zeyu Zhang</em>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors"> </i> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2020-icra-ar-workspace/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/439150333" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/Janetalready/Shared-AR-workspace" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/view/shared-ar-workspace/" target="_blank" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener">Website</a> </div> <div class="abstract hidden"> <p>We design and develop a new shared Augmented Reality (AR) workspace for Human-Robot Interaction (HRI), which establishes a bi-directional communication between human agents and robots. In a prototype system, the shared AR workspace enables a shared perception, so that a physical robot not only perceives the virtual elements in its own view but also infers the utility of the human agent—the cost needed to perceive and interact in AR—by sensing the human agent’s gaze and pose. Such a new HRI design also affords a shared manipulation, wherein the physical robot can control and alter virtual objects in AR as an active agent; crucially, a robot can proactively interact with human agents, instead of purely passively executing received commands. In experiments, we design a resource collection game that qualitatively demonstrates how a robot perceives, processes, and manipulates in AR and quantitatively evaluates the efficacy of HRI using the shared AR workspace. We further discuss how the system can potentially benefit future HRI studies that are otherwise challenging.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">qiu2020human</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-robot Interaction in a Shared Augmented Reality Workspace}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Qiu, Shuwen and Liu, Hangxin and Zhang, Zeyu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{11413--11418}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#28a745"> <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/papers/2019-icra-ssl/thumbnail.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2019-icra-ssl/thumbnail.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2019self" class="col-sm-8"> <div class="title">Self-supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment</div> <div class="tags" style="margin-bottom: 0.5rem;"> <code style="background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 1.5px 5px; margin-right: 3.5px; margin-bottom: 2px; border-radius: 2.5px; font-size: 0.8em; color: #495057; display: inline-block;">Teaming</code> </div> <div class="author"> <a href="https://liuhx111.github.io" rel="external nofollow noopener" target="_blank">Hangxin Liu<sup>*</sup></a>, <em>Zeyu Zhang<sup>*</sup></em>, <a href="https://yzhu.io" rel="external nofollow noopener" target="_blank">Yixin Zhu</a>, and <a href="http://www.stat.ucla.edu/~sczhu/index.html" rel="external nofollow noopener" target="_blank">Song-Chun Zhu</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* denotes joint first authors"> </i> </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/papers/2019-icra-ssl/paper.pdf" target="_blank" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://vimeo.com/321150838" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/TooSchoolForCool/EddieBot-ROS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper presents an incremental learning framework for mobile robots localizing the human sound source using a microphone array in a complex indoor environment consisting of multiple rooms. In contrast to conventional approaches that leverage direction-of-arrival estimation, the framework allows a robot to accumulate training data and improve the performance of the prediction model over time using an incremental learning scheme. Specifically, we use implicit acoustic features obtained from an auto-encoder together with the geometry features from the map for training. A self-supervision process is developed such that the model ranks the priority of rooms to explore and assigns the ground truth label to the collected data, updating the learned model on-the-fly. The framework does not require pre-collected data and can be directly applied to real-world scenarios without any human supervisions or interventions. In experiments, we demonstrate that the prediction accuracy reaches 67% using about 20 training samples and eventually achieves 90% accuracy within 120 samples, surpassing prior classification-based methods with explicit GCC-PHAT features.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2019self</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Hangxin and Zhang, Zeyu and Zhu, Yixin and Zhu, Song-Chun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2599--2605}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zeyu Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>